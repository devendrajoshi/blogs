<head>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css" rel="stylesheet" />
</head>

<h1>Building a RAG-Powered API with FastAPI and OllamaLLM</h1>

<body>
<h2>Introduction</h2>

<p>Consider a doctor diagnosing a patient based on their symptoms. In one scenario, the doctor, lacking any knowledge about the patient's medical history or other relevant information, makes a diagnosis. It might be accurate, but there's a chance it could be more precise.</p>

<p>In another scenario, the doctor has access to a comprehensive patient information system containing all the patient's past medical records and other pertinent data. The diagnosis in this case is likely to be much more informed.</p>

<p>This analogy, while not perfect, helps explain a concept in AI. Here, the doctor represents a <b>Large Language Model (LLM)</b>, and the patient information system symbolizes a document retrieval system. Their combination forms what we call <b>Retrieval Augmented Generation (RAG)</b>.</p>

<p>RAG harnesses the power of LLMs and document retrieval to deliver more accurate and context-specific responses. It's particularly useful for tasks like question answering and summarization. However, the actual implementation and use cases of RAG can be far more complex and varied than this analogy suggests.</p>

<p>Delving deeper into these concepts, RAG merges the strengths of two potent AI techniques: Large Language Models and Document Retrieval Systems. LLMs, trained on extensive text data, can generate human-like text based on the input they receive. Document Retrieval Systems, on the other hand, can sift through large databases of documents to find the most relevant information based on a query.</p>

<p>When combined in RAG, these two techniques result in an AI system that can generate relevant and coherent responses, and also pull in specific information from a large database of documents to provide more accurate and context-specific answers.</p>

<p>For more detailed information on these concepts, consider the following resources:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Language_model">Large Language Models</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Document_retrieval">Document Retrieval Systems</a></li>
  <li><a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation</a></li>
</ul>

<p>Remember, the field of AI is dynamic, and while RAG is a powerful tool today, the future may hold even more exciting developments!</p>


<h2>Setting Up the Environment</h2>

<h2>Setting Up the Environment</h2>

<p>In this project, we are using Docker to set up our environment. Docker allows us to package our application with all of its dependencies into a standardized unit for software development. Below is the configuration of our docker-compose.yml file:</p>

<pre><code class="language-yaml">
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-container
    volumes:
      - ${HOST_OLLAMA_HOME}:/root/.ollama
    restart: always

  fastapi:
    build: .
    container_name: fastapi-container
    restart: always
    depends_on:
      - ollama
    ports:
      - "9001:9001"
    volumes:
      - ${HOST_INDEX_PATH}:${INDEX_PATH}
      - ${HOST_DOCS_PATH}:${LOCAL_DOCS_PATH}
</code></pre>

<p>This setup includes two main services: <b>ollama</b> and <b>fastapi</b>. The <b>ollama</b> service is responsible for interacting with the LLM. We are using the official image of ollama, which ensures that we have the most stable and up-to-date version of the service. This eliminates the need for us to rebuild the image every time there is an update.</p>

<p>The <b>fastapi</b> service handles the REST API. We are building this image ourselves, as specified in the Dockerfile. This allows us to customize the service according to our needs. The Dockerfile includes instructions to install the necessary dependencies, copy our application into the container, and set up the environment for our application.</p>

<p>Here is the Dockerfile that we use to build the fastapi service:</p>

<pre><code class="language-dockerfile">
FROM python:3.12

RUN apt-get update && apt-get install -y ca-certificates 

EXPOSE 9001

# Keeps Python from generating .pyc files in the container
ENV PYTHONDONTWRITEBYTECODE=1

# Turns off buffering for easier container logging
ENV PYTHONUNBUFFERED=1

# Install pip requirements
COPY requirements.txt .
RUN python -m pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org -r requirements.txt

# install gunicorn
RUN pip install --trusted-host files.pythonhosted.org --trusted-host pypi.org --trusted-host pypi.python.org gunicorn

WORKDIR /app
COPY ./app /app

# Create folders localdocs and index in app
RUN mkdir /app/localdocs
RUN mkdir /app/index

# Add /app to PYTHONPATH
ENV PYTHONPATH=/app:$PYTHONPATH

# Creates a non-root user with an explicit UID and adds permission to access the /app folder
RUN adduser -u 5678 --disabled-password --gecos "" appuser && chown -R appuser /app
USER appuser

CMD ["gunicorn", "--bind", "0.0.0.0:9001", "-k", "uvicorn.workers.UvicornWorker", "main:app"]
</code></pre>

<p>We're also using Chroma as our vector database within the fastapi container. This setup ensures that our application is modular, scalable, and easy to maintain.</p>

<p>We have a few volume mounts for persisting and reusing data. These paths are defined in a .env file. Here's an example:</p>

<pre><code class="language-bash">
#Path inside container
INDEX_PATH=/app/index/

#Path on host (start with an empty directory)
HOST_INDEX_PATH=/path/to/your/index/

LLM_MODEL=llama3.2:1b
LLM_HOST=ollama
LLM_PORT=11434
HOST_OLLAMA_HOME=/path/to/your/ollama_home/

EMBEDDING_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
SPLITTER_CHUNK_SIZE=1000
SPLITTER_CHUNK_OVERLAP=200

#Path on host. Create a folder and put all your PDF in this folder on which to create embeddings
HOST_DOCS_PATH=/path/to/your/localdocs/
#Path inside container
LOCAL_DOCS_PATH=/docs/
</code></pre>

<h2>Defining the FastAPI Application</h2>
<p>We start by defining our FastAPI application:</p>
<pre><code class="language-python">
app = FastAPI(
    title = "RAG Tutorial",
    version ="1.0",
    description ="A simple API Server"
)
</code></pre>
  
<p>This creates a new FastAPI application with the specified title, version, and description. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p>
<h2>Adding Middleware</h2>
<p>Next, we add a middleware to our application:</p>
<pre><code class="language-python">
app.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=1)
</code></pre>
<p>This line adds the GZipMiddleware to our FastAPI application. Middleware is a way to add extra functionality to the request/response processing flow. In this case, the GZipMiddleware will compress HTTP responses for all routes.</p>
<h2>Defining the Request Model</h2>
<p>We define a Pydantic model to validate the data we receive in our endpoint:</p>
<pre><code class="language-python">
class RequestModel(BaseModel):
    #prompt is mandatory
    prompt: str
    session_context: Optional[str] = Field(None)
</code></pre>
  
<p>This model expects a mandatory ‘prompt’ field and an optional ‘session_context’ field in the incoming request.</p>
<h2>Creating the Index Endpoint</h2>
<p>We define an endpoint for creating the index:</p>
<pre><code class="language-python">
@app.post("/create_index/")
async def create_index_endpoint(background_tasks: BackgroundTasks):
    global is_index_creation_running
    if is_index_creation_running:
        return {"message":"Index creation is already running"}
    else:
        is_index_creation_running = True
        background_tasks.add_task(create_index_background)
        return {"message":"Index creation started in background"}  
</code></pre>
<p>This endpoint starts the index creation in the background. If the index creation is already running, it returns a message indicating that. Otherwise, it starts the index creation and returns a message indicating that the index creation has started.</p>
<h2>Creating the Query Endpoint</h2>
<p>Finally, we define the main query endpoint:</p>
<pre><code class="language-python">
@app.post("/query/")
async def generate_response(request: RequestModel):
    ...
</code></pre>
<p>This endpoint takes a RequestModel as input and generates a response. If the vector database is not None, it uses the retriever to get documents related to the prompt. If there are no documents, it generates a response without using the retriever. Otherwise, it uses the retriever and the langchain model to generate a response.</p>
<h2>References</h2>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-bash.min.js"></script>
</body>
