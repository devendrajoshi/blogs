<head>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css" rel="stylesheet" />
</head>

<h1>Building a RAG-Powered API with FastAPI and OllamaLLM</h1>

<body>
<h2>Introduction</h2>

<p>Let's start with an analogy. Imagine a doctor who has to diagnose a patient based on their symptoms. In one scenario, the doctor knows nothing about the patient's medical history, family history, or any other relevant information. The doctor can still make a diagnosis, but it might not be as accurate or helpful as it could be.</p>

<p>Now, imagine a second scenario where the doctor has access to a comprehensive patient information system. This system contains all the patient's past medical records, family history, and other relevant data. The doctor can now make a much more informed diagnosis.</p>

<p>This analogy is used to explain a concept in the world of AI, and while it may not perfectly align with every scenario, it provides a helpful visualization. In this context, the doctor is akin to a <b>Large Language Model (LLM)</b>, and the patient information system is like a document retrieval system. The combination of these two is what we call <b>Retrieval Augmented Generation (RAG)</b>.</p>

<p>RAG leverages the power of LLMs and document retrieval to provide more accurate and context-specific responses, making it ideal for tasks like question answering and summarization. It's important to note that while this analogy helps to understand the concept, the actual implementation and use cases of RAG can be far more complex and varied.</p>

<p>To dig deeper into these concepts, let's break down how RAG works. At its core, RAG combines the strengths of two powerful AI techniques: Large Language Models and Document Retrieval Systems. LLMs are trained on vast amounts of text data, enabling them to generate human-like text based on the input they receive. On the other hand, Document Retrieval Systems can search through large databases of documents to find the most relevant information based on a query.</p>

<p>When these two are combined in RAG, the result is an AI system that can not only generate relevant and coherent responses, but also pull in specific information from a large database of documents to provide more accurate and context-specific answers. This makes RAG particularly useful for tasks like question answering and summarization where context and specificity are key.</p>

<p>For more detailed information on these concepts, you can refer to the following resources:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Language_model">Large Language Models</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Document_retrieval">Document Retrieval Systems</a></li>
  <li><a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation</a></li>
</ul>

<p>Remember, the field of AI is constantly evolving, and while RAG is a powerful tool today, who knows what exciting developments the future will bring!</p>


<h2>Setting Up the Environment</h2>

<p>We will be using Docker to set up our environment. Here's what our docker-compose.yml file will look like:</p>

<pre><code class="language-yaml">
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-container
    volumes:
      - ${HOST_OLLAMA_HOME}:/root/.ollama
    restart: always

  fastapi:
    build: .
    container_name: fastapi-container
    restart: always
    depends_on:
      - ollama
    ports:
      - "9001:9001"
    volumes:
      - ${HOST_INDEX_PATH}:${INDEX_PATH}
      - ${HOST_DOCS_PATH}:${LOCAL_DOCS_PATH}
</code></pre>

<p>This setup includes two main services: <b>ollama</b> and <b>fastapi</b>. The <b>ollama</b> service is responsible for interacting with the LLM, while the <b>fastapi</b> service handles the REST API. We're also using Chroma as our vector database within the fastapi container.</p>

<p>We have a few volume mounts for persisting and reusing data. These paths are defined in a .env file. Here's an example:</p>

<pre><code class="language-bash">
#Path inside container
INDEX_PATH=/app/index/

#Path on host (start with an empty directory)
HOST_INDEX_PATH=/path/to/your/index/

LLM_MODEL=llama3.2:1b
LLM_HOST=ollama
LLM_PORT=11434
HOST_OLLAMA_HOME=/path/to/your/ollama_home/

EMBEDDING_MODEL_NAME=sentence-transformers/all-mpnet-base-v2
SPLITTER_CHUNK_SIZE=1000
SPLITTER_CHUNK_OVERLAP=200

#Path on host. Create a folder and put all your PDF in this folder on which to create embeddings
HOST_DOCS_PATH=/path/to/your/localdocs/
#Path inside container
LOCAL_DOCS_PATH=/docs/
</code></pre>

<h2>Defining the FastAPI Application</h2>
<p>We start by defining our FastAPI application:</p>
<pre><code class="language-python">
app = FastAPI(
    title = "RAG Tutorial",
    version ="1.0",
    description ="A simple API Server"
)
</code></pre>
  
<p>This creates a new FastAPI application with the specified title, version, and description. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p>
<h2>Adding Middleware</h2>
<p>Next, we add a middleware to our application:</p>
<pre><code class="language-python">
app.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=1)
</code></pre>
<p>This line adds the GZipMiddleware to our FastAPI application. Middleware is a way to add extra functionality to the request/response processing flow. In this case, the GZipMiddleware will compress HTTP responses for all routes.</p>
<h2>Defining the Request Model</h2>
<p>We define a Pydantic model to validate the data we receive in our endpoint:</p>
<pre><code class="language-python">
class RequestModel(BaseModel):
    #prompt is mandatory
    prompt: str
    session_context: Optional[str] = Field(None)
</code></pre>
  
<p>This model expects a mandatory ‘prompt’ field and an optional ‘session_context’ field in the incoming request.</p>
<h2>Creating the Index Endpoint</h2>
<p>We define an endpoint for creating the index:</p>
<pre><code class="language-python">
@app.post("/create_index/")
async def create_index_endpoint(background_tasks: BackgroundTasks):
    global is_index_creation_running
    if is_index_creation_running:
        return {"message":"Index creation is already running"}
    else:
        is_index_creation_running = True
        background_tasks.add_task(create_index_background)
        return {"message":"Index creation started in background"}  
</code></pre>
<p>This endpoint starts the index creation in the background. If the index creation is already running, it returns a message indicating that. Otherwise, it starts the index creation and returns a message indicating that the index creation has started.</p>
<h2>Creating the Query Endpoint</h2>
<p>Finally, we define the main query endpoint:</p>
<pre><code class="language-python">
@app.post("/query/")
async def generate_response(request: RequestModel):
    ...
</code></pre>
<p>This endpoint takes a RequestModel as input and generates a response. If the vector database is not None, it uses the retriever to get documents related to the prompt. If there are no documents, it generates a response without using the retriever. Otherwise, it uses the retriever and the langchain model to generate a response.</p>
<h2>References</h2>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-bash.min.js"></script>
</body>
