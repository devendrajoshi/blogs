<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a RAG-Powered API with FastAPI and OllamaLLM: A Hands-On Tutorial</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            background-color: #2196F3;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
            padding: 20px;
            background: white;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #333;
        }
        p {
            margin: 10px 0;
        }
        ul {
            margin: 10px 0;
            padding: 0 20px;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
        }
    </style>
</head>
<body>
    <header>
        <h1>Building a RAG-Powered API with FastAPI and OllamaLLM: A Hands-On Tutorial</h1>
    </header>
    <div class="container">
        <p>In this blog post, we'll walk you through the process of building a RAG-powered API using FastAPI and OllamaLLM. We'll start by explaining what RAG is and how it works. Then, we'll dive into the code, demonstrating how to set up the API, create an embeddings index, and use RAG to generate responses.</p>
        
        <h2>What is RAG?</h2>
        <p>Imagine you're a doctor trying to diagnose a patient. Without access to Patient Information Systems (PIS), you might rely solely on your intuition and experience. However, with the aid of a PIS, you can access the patient's medical history, family history, and current symptoms, leading to a more accurate diagnosis.</p>
        <p>In this analogy, the doctor represents a Large Language Model (LLM), and the PIS represents a document vector store. RAG (Retrieval Augmented Generation) is the process of combining these two elements to produce more accurate and informative responses. It's like the doctor using the PIS to provide a more tailored and relevant diagnosis.</p>
        
        <p>Delving deeper into these concepts, RAG merges the strengths of two potent AI techniques: Large Language Models and Document Retrieval Systems. LLMs, trained on extensive text data, can generate human-like text based on the input they receive. Document Retrieval Systems, on the other hand, can sift through large databases of documents to find the most relevant information based on a query.</p>
        
        <p>When combined in RAG, these two techniques result in an AI system that can generate relevant and coherent responses, and also pull in specific information from a large database of documents to provide more accurate and context-specific answers.</p>
        
        <p>An important concept in RAG is <b>embedding</b>. Embeddings are numerical representations of text that capture the semantic meaning of words, phrases, or documents. By converting text into embeddings, we can perform efficient similarity searches and retrieve relevant information from large datasets. Embeddings allow the system to understand and compare the meaning of different pieces of text, making it possible to find the most relevant documents for a given query.</p>
        
        <p>There are several variants of RAG, including:</p>
        <ul>
            <li><b><a href="https://haystack.deepset.ai/cookbook/using_hyde_for_improved_retrieval">HyDE (Hypothetical Document Embedding)</a>:</b> This method generates mock documents to improve the performance of document retrieval in a Haystack pipeline. It is especially useful for data from a special domain. HyDE first generates a hypothetical document based on the query, then encodes it into an embedding vector and uses it to identify similar actual documents in the corpus.</li>
            <li><b><a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/">Graph RAG</a>:</b> This approach uses graph databases to represent relationships between documents and entities, allowing for more complex queries.</li>
        </ul>
        
        <p>For more detailed information on these concepts, consider the following resources:</p>
        <ul>
            <li><a href="https://en.wikipedia.org/wiki/Language_model">Large Language Models</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Document_retrieval">Document Retrieval Systems</a></li>
            <li><a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation</a></li>
        </ul>
        
        <p>Remember, the field of AI is dynamic, and while RAG is a powerful tool today, the future may hold even more exciting developments!</p>
        
        <h2>Building a RAG-Powered API with FastAPI and OllamaLLM</h2>
        <p>For this project, we'll be using FastAPI to create a REST API that will expose our RAG-powered functionality. We'll also use OllamaLLM to interact with our local LLM models.</p>
        
        <h3>Why Local LLM?</h3>
        <p>Using a local LLM has several advantages and disadvantages:</p>
        <ul>
            <li><b>Pros:</b>
                <ul>
                    <li>Data Privacy: Local LLMs ensure that sensitive data does not leave your premises.</li>
                    <li>Customization: You can fine-tune the model to better suit your specific needs.</li>
                    <li>Latency: Local models can reduce latency as they do not require network calls to external servers.</li>
                </ul>
            </li>
            <li><b>Cons:</b>
                <ul>
                    <li>Resource Intensive: Running LLMs locally requires significant computational resources.</li>
                    <li>Maintenance: You are responsible for maintaining and updating the model.</li>
                    <li>Scalability: Scaling local models can be more challenging compared to cloud-based solutions.</li>
                </ul>
            </li>
        </ul>
        
        <p>OllamaLLM is a powerful and flexible LLM platform that allows you to easily deploy and manage LLM models. It provides a simple API for interacting with the models, making it easy to integrate LLMs into your applications.</p>
        <p>FastAPI is a high-performance Python web framework that is ideal for building APIs. It is easy to learn and use, and it provides a number of features that make it a great choice for building REST APIs.</p>
        <p>Our architecture will utilize Docker containers to make it easier to manage and deploy our application. Docker containers provide a portable and isolated environment for running applications, making it easier to develop, test, and deploy our RAG-powered API.</p>
        
        <h2>System Requirements:</h2>
        <p>I developed this project on a MacBook with a 2.3 GHz Quad-Core Intel Core i7 and 16 GB 3733 MHz LPDDR4X. The response times were slow on this setup. For better performance, I recommend using an EC2 instance. I tested it on an EC2 instance with the following specifications:</p>
        <ul>
            <li>Instance Type: g4dn.4xlarge (16 vCPUs, 64 GiB RAM, 1 x NVIDIA T4 GPU)</li>
            <li>AMI: Deep Learning AMI (Ubuntu 18.04) Version 36.0 (ami-0729c079aae647cb3)</li>
        </ul>
        
        <h2>Running the Code:</h2>
        <p>The complete code for this project is available on GitHub at <a href="https://github.com/devendrajoshi/rag.git">https://github.com/devendrajoshi/rag.git</a>.</p>
        <p>To follow along with the code, you'll need to clone the repository and run the Docker containers. Here's how:</p>
        <ol>
            <li><b>Clone the Repository:</b></li>
            <pre><code class="language-bash">git clone https://github.com/devendrajoshi/rag.git</code></pre>
            <li><b>Navigate to the project directory:</b></li>
            <pre><code class="language-bash">cd rag</code></pre>
            <li><b>Start the Docker containers:</b></li>
            <pre><code class="language-bash">docker-compose up -d</code></pre>
        </ol>
        <p>This command will clone the repository and
